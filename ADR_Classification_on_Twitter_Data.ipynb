{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "vWIAcTVXBlow",
    "outputId": "7eb357f4-5a0e-4455-dbf7-809768bd360a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import numpy\n",
    "import sys\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb4lyYJ5lg6r"
   },
   "source": [
    "#Please check the path of the folder 'Dataset' before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDvRORkCanc2"
   },
   "outputs": [],
   "source": [
    "folder_path=\"./Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3n7knKXag5v"
   },
   "outputs": [],
   "source": [
    "#Retrieving files from drive\n",
    "\n",
    "umls_semantic_types_files = folder_path + \"/UMLS Semantic File Types.txt\"\n",
    "w2v_model_file = folder_path + \"Health_2.5mreviews.s200.w10.n5.v15.cbow.bin\"\n",
    "W2V_FEATURES_NUM = 200\n",
    "mpqa_lexicon = folder_path + \"/subjclueslen1-HLTEMNLP05.txt\"\n",
    "senti_word_net = folder_path + \"/SentiWordNet_3.0.0.txt\"\n",
    "negative_words = folder_path + \"/negative_words.txt\"\n",
    "sentiment_score_dict = folder_path + \"/sentiment_score_dic_with_pmi.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrzJqw0uBxDG"
   },
   "outputs": [],
   "source": [
    "#Extract Window Word Features\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ExtractWindowWordsFeature(BaseEstimator):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_feature_names(self):\n",
    "    return 'extract_window_words'\n",
    "\n",
    "  def extract_window_words(self, entities):\n",
    "    window_words = []\n",
    "\n",
    "    for entity in entities:\n",
    "      window_words.append(entity['text'])\n",
    "\n",
    "    return window_words\n",
    "\n",
    "  def fit(self, entities, y=None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, entities):\n",
    "    print(\"extract word transform\")\n",
    "    return self.extract_window_words(entities)\n",
    "\n",
    "  def fit_transform(self, entities, y= None):\n",
    "    print (\"extract word fit_transform\")\n",
    "    return self.extract_window_words(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMPsqKm-CuCp"
   },
   "outputs": [],
   "source": [
    "#PoS Tag Features\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from pandas import DataFrame\n",
    "\n",
    "class PosTagFeatures(BaseEstimator):\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_feature_names(self):\n",
    "    return 'pos_tag'\n",
    "  \n",
    "  def fit(self, documents, y=None):\n",
    "    return self\n",
    "\n",
    "  def find_pos_tag_index(self, pos_tag, pos_dict):\n",
    "    index = 0\n",
    "    \n",
    "    for pos in pos_dict:\n",
    "      if pos.find(pos_tag[1]) != -1:\n",
    "        return index\n",
    "\n",
    "      index += 1\n",
    "\n",
    "    return -1\n",
    "  \n",
    "  def transform(self, window_words, y=None):\n",
    "    features = []\n",
    "    pos_dict = [\"NN\", \"JJ\", \"VB\", \"RB\"]\n",
    "\n",
    "    for window_word in window_words:\n",
    "      feature = [0]*len(pos_dict)\n",
    "      tokens = nltk.tokenize.word_tokenize(window_word)\n",
    "      pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "      for pos_tag in pos_tags:\n",
    "        index = self.find_pos_tag_index(pos_tag, pos_dict)\n",
    "\n",
    "        if index != -1:\n",
    "          feature[index] += 1\n",
    "        \n",
    "      features.append(feature)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIPFTE6TCy-Q"
   },
   "outputs": [],
   "source": [
    "#Sentiment Features\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "#from dill.dill import FileNotFoundError\n",
    "from pandas import DataFrame\n",
    "from scipy import sparse\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class SentimentFeature(BaseEstimator):\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_feature_names(self):\n",
    "    return 'sentiment'\n",
    "\n",
    "  def fit(self, documents, y=None):\n",
    "    return self\n",
    "\n",
    "  def load_sentiment_dict_subj(self):\n",
    "    sentiment_dict = {}\n",
    "    f = open(mpqa_lexicon)\n",
    "    \n",
    "    for line in f:\n",
    "      terms = line.split(\" \")\n",
    "      sentiment_dict[terms[2][terms[2].index(\"=\") + 1:]] = terms[len(terms) - 1][terms[len(terms) - 1].index(\"=\") + 1:].strip()\n",
    "        \n",
    "    return sentiment_dict\n",
    "\n",
    "  def load_sentiment_dict_word_net(self):\n",
    "    result = {}\n",
    "    f = open(senti_word_net)\n",
    "    \n",
    "    for line in f:\n",
    "      if not (line.startswith(\"#\") or line.startswith(\";\")):\n",
    "        line_parts = line.split(\"\\t\")\n",
    "        pos_score = line_parts[2]\n",
    "        neg_score = line_parts[3]\n",
    "        syn_terms_split = line_parts[4].split(\" \")\n",
    "        for syn_term_split in syn_terms_split:\n",
    "          result[syn_term_split.split(\"#\")[0]] = {'pos_score' : float(pos_score), 'neg_score' : abs(float(neg_score))}\n",
    "                        \n",
    "    return result\n",
    "\n",
    "  def create_sentiment_feature_word_net(self, sentiment_dict, window_word, negative_words, punctuation):\n",
    "    feature = [0] * 8\n",
    "    zero_pmi_score = 0\n",
    "    total_score = 0\n",
    "    max_score = 0\n",
    "    last_score = 0\n",
    "    zero_pmi_score_neg = 0\n",
    "    total_score_neg = 0\n",
    "    max_score_neg = 0\n",
    "    last_score_neg = 0\n",
    "    is_context_negated = False\n",
    "    \n",
    "    for word in window_word:\n",
    "      if word in negative_words:\n",
    "        is_context_negated = True\n",
    "      elif word in punctuation:\n",
    "        is_context_negated = False\n",
    "            \n",
    "      if word in sentiment_dict:\n",
    "        sentiment = sentiment_dict[word]\n",
    "        pos = float(sentiment['pos_score'])\n",
    "        neg = float(sentiment['neg_score'])\n",
    "        score = pos - neg\n",
    "        \n",
    "        if is_context_negated:\n",
    "          if score != 0:\n",
    "            zero_pmi_score_neg += 1\n",
    "            \n",
    "          total_score_neg += score\n",
    "            \n",
    "          if score != 0 and (score > max_score_neg or (abs(score) > max_score_neg and max_score_neg == 0)):\n",
    "            max_score_neg = score\n",
    "              \n",
    "          last_score_neg = score\n",
    "          \n",
    "        else:\n",
    "          if score != 0:\n",
    "            zero_pmi_score += 1\n",
    "            \n",
    "            if score > max_score or ( abs(score) > max_score and max_score == 0):\n",
    "              max_score = score\n",
    "              \n",
    "            last_score = score\n",
    "            \n",
    "          total_score += score\n",
    "        \n",
    "    feature[0] = zero_pmi_score\n",
    "    feature[1] = total_score\n",
    "    feature[2] = max_score\n",
    "    feature[3] = last_score\n",
    "    feature[4] = zero_pmi_score_neg\n",
    "    feature[5] = total_score_neg\n",
    "    feature[6] = max_score_neg\n",
    "    feature[7] = last_score_neg\n",
    "    \n",
    "    return feature\n",
    "\n",
    "\n",
    "  def load_negated_words(self):\n",
    "    f = open(negative_words)\n",
    "    negative_words_dict = []\n",
    "\n",
    "    for line in f:\n",
    "      negative_words_dict.append(line.strip())\n",
    "        \n",
    "    return negative_words_dict\n",
    "\n",
    "  def create_sentiment_feature_subj(self, sentiment_dict, window_word, negative_words, punctuation):\n",
    "    feature = [0] * 4\n",
    "    positive_affirmative = 0\n",
    "    negative_affirmative = 0\n",
    "    positive_negated = 0\n",
    "    negative_negated = 0\n",
    "    is_negative_context = False\n",
    "        \n",
    "    for word in window_word:\n",
    "      if word in negative_words:\n",
    "        is_negative_context = True\n",
    "      elif word in punctuation:\n",
    "        is_negative_context = False\n",
    "            \n",
    "      if word in sentiment_dict:\n",
    "        sentiment = sentiment_dict[word].strip()\n",
    "                \n",
    "        if sentiment == \"negative\":\n",
    "          if is_negative_context:\n",
    "            negative_negated += 1\n",
    "          else:\n",
    "            negative_affirmative += 1\n",
    "                \n",
    "        elif sentiment == \"positive\":\n",
    "          if is_negative_context:\n",
    "            positive_negated += 1\n",
    "          else:\n",
    "            positive_affirmative += 1\n",
    "        \n",
    "    feature[0] = positive_affirmative\n",
    "    feature[1] = negative_affirmative\n",
    "    feature[2] = positive_negated\n",
    "    feature[3] = negative_negated\n",
    "    \n",
    "    return feature\n",
    "\n",
    "    \n",
    "  def transform(self, window_words, y=None):\n",
    "    features = []\n",
    "    sentiment_dict_subj = self.load_sentiment_dict_subj()\n",
    "    sentiment_dict_word_net = self.load_sentiment_dict_word_net()\n",
    "    negative_words = self.load_negated_words()\n",
    "    punctuation = [',', '.', '!', '?']\n",
    "    \n",
    "    for window_word in window_words:\n",
    "      window_word_parts = nltk.word_tokenize(window_word)\n",
    "      feature_sent_word_net = self.create_sentiment_feature_word_net(sentiment_dict_word_net, window_word_parts, negative_words, punctuation)\n",
    "      feature_sent_subj = self.create_sentiment_feature_subj(sentiment_dict_subj, window_word_parts, negative_words, punctuation)\n",
    "      feature = []\n",
    "      feature.extend(feature_sent_word_net)\n",
    "      feature.extend(feature_sent_subj)\n",
    "      features.append(feature)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSzj1FzSHw33"
   },
   "outputs": [],
   "source": [
    "#Sentiment Score with PMI\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class SentimentScore(BaseEstimator):\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_feature_names(self):\n",
    "    return \"sentiment_score\"\n",
    "\n",
    "  def fit(self, documents, y=None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, window_words, y=None):\n",
    "    f = open(sentiment_score_dict)\n",
    "    word_dict = {}\n",
    "\n",
    "    for line in f:\n",
    "      line_parts = line.split('\\t')\n",
    "      word_dict[line_parts[0]] = float(line_parts[3])\n",
    "\n",
    "    features = []\n",
    "    i=0\n",
    "    max_len = 0\n",
    "\n",
    "    for window_word in window_words:\n",
    "      window_word_parts = window_word.split(\" \")\n",
    "      \n",
    "      if len(window_word_parts) > max_len:\n",
    "        max_len = len(window_word_parts)\n",
    "\n",
    "    print(\"Sentiment score = \" + str(max_len))\n",
    "\n",
    "    for window_word in window_words:\n",
    "      window_word_parts = window_word.split(\" \")\n",
    "      feature = [0]\n",
    "      max=0\n",
    "      min=0\n",
    "\n",
    "      for word in window_word_parts:\n",
    "        if word in word_dict:\n",
    "          pmi = word_dict[word]\n",
    "          feature[0] += pmi\n",
    "      \n",
    "      features.append(feature)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEUdYzzJILMX"
   },
   "outputs": [],
   "source": [
    "#Entities Feature\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ExtractEntitiesFeature(BaseEstimator):\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_feature_name(self):\n",
    "    return \"extract_entities\"\n",
    "\n",
    "  def extract_entities(self, entities):\n",
    "    entity_text = []\n",
    "\n",
    "    for entity in entities:\n",
    "      entity_text.append(entity['entity'])\n",
    "\n",
    "    return entity_text\n",
    "\n",
    "  def fit(self, entities, y=None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, entities):\n",
    "    return self.extract_entities(entities)\n",
    "\n",
    "  def fit_transform(self, entities, y= None):\n",
    "    return self.extract_entities(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWMhDrGSIL8Z"
   },
   "outputs": [],
   "source": [
    "# Word2Vec Features\n",
    "\n",
    "import nltk\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "class W2VFeatures(BaseEstimator):\n",
    "\n",
    "  def __init__(self, model = None, num_features = None):\n",
    "    self.model = model\n",
    "    self.num_features = num_features\n",
    "    pass\n",
    "\n",
    "  def get_feature_name(self):\n",
    "    return \"w2v\"\n",
    "\n",
    "  def fit(self, entities, y=None):\n",
    "    return self\n",
    "\n",
    "  def makeFeatureVec(self, words, model, num_features):\n",
    "    featureVec = numpy.zeros((num_features,), dtype = \"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.index2word)\n",
    "\n",
    "    for word in words:\n",
    "      if word in index2word_set:\n",
    "        nwords +=1\n",
    "        featureVec = numpy.add(featureVec, model[word])\n",
    "\n",
    "    featureVec = numpy.divide(featureVec, nwords)\n",
    "\n",
    "    return featureVec\n",
    "\n",
    "  def getAvgFeatureVecs(self, reviews, model, num_features):\n",
    "    counter = 0.\n",
    "    wout = codecs.open(\"PubMed-and-PMC-w2v_mesh_words.txt\", \"w\", encoding = \"utf-8\")\n",
    "    reviewFeatureVecs = numpy.zeros((len(reviews), num_features), dtype = \"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "      clean_train_reviews =[]\n",
    "      w = re.sub(\"\\+\", \" \", review)\n",
    "      clean_train_reviews = [w.split()]\n",
    "\n",
    "      for word in clean_train_reviews:\n",
    "        if counter%1000 == 0:\n",
    "          print(\"Review %d of %d\" %(counter, len(reviews)))\n",
    "\n",
    "        vec1 = self.makeFeatureVec(word, model, self.num_features)\n",
    "\n",
    "        reviewFeatureVecs[counter] = vec1\n",
    "      \n",
    "      vec1 = [str(l) for l in vec1.tolist()]\n",
    "      print(vec1)\n",
    "\n",
    "      wout.write(review + \"\\t\" + \"\\t\".join(vec1) + \"\\n\")\n",
    "      counter+=1\n",
    "\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "  def transform(self, entities):\n",
    "    return self.create_w2v_feature(entities)\n",
    "\n",
    "  def create_w2v_feature(self, entities):\n",
    "    features = []\n",
    "\n",
    "    for entity in entities:\n",
    "      text = entity\n",
    "      words = nltk.word_tokenize(text)\n",
    "      feature = self.makeFeatureVec(words, self.model, self.num_features)\n",
    "      feature_clear = []\n",
    "\n",
    "      for x in feature:\n",
    "        if np.math.isnan(x):\n",
    "          feature_clear.append(0.0)\n",
    "        else:\n",
    "          feature_clear.append(x)\n",
    "\n",
    "      features.append(feature_clear)\n",
    "\n",
    "    return features\n",
    "\n",
    "  def convert_to_list(self, features):\n",
    "    result = []\n",
    "\n",
    "    for feature in features:\n",
    "      result.append(list(list(feature.toarray())[0]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "  def fit_transform(self, entities, y=None):\n",
    "    return self.create_w2v_feature(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkmJK7HlKYJv"
   },
   "outputs": [],
   "source": [
    "#UMLS Semantic Type Features\n",
    "\n",
    "import nltk\n",
    "from pandas import DataFrame\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class UMLSemanticTypeFeature(BaseEstimator):\n",
    "\n",
    "  def __init__(self, semantic_types):\n",
    "    self.semantic_types = semantic_types\n",
    "    pass\n",
    "\n",
    "  def get_feature_names(self):\n",
    "    return \"umls_semantic_types\"\n",
    "\n",
    "  def create_semantic_type_features(self,entities):\n",
    "    features = []\n",
    "    semantic_types_set = set()\n",
    "    keys = self.semantic_types.keys()\n",
    "\n",
    "    for key in keys:\n",
    "      semantic_types_set.add(self.semantic_types[key])\n",
    "    \n",
    "    cluster_number_list = list(semantic_types_set)\n",
    "\n",
    "    for entity in entities:\n",
    "      feature = [0]*len(self.semantic_types)\n",
    "\n",
    "      if entity in self.semantic_types:\n",
    "        semantic_type = self.semantic_types[entity]\n",
    "        feature[cluster_number_list.index(semantic_type)] = 1\n",
    "\n",
    "      features.append(feature)\n",
    "\n",
    "    return features\n",
    "\n",
    "  def fit(self, documents, y=None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, entities):\n",
    "    return self.create_semantic_type_features(entities)\n",
    "\n",
    "  def fit_transform(self, entities, y=None):\n",
    "    return self.create_semantic_type_features(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtyC20EcIlyW"
   },
   "outputs": [],
   "source": [
    "#Load UMLS Semantic Types\n",
    "\n",
    "def load_umls_semantic_types():\n",
    "  f = open(umls_semantic_types_files)\n",
    "  dict_cluss = {}\n",
    "  \n",
    "  for line in f:\n",
    "    terms = line.split(\"\\t\")\n",
    "    dict_cluss[terms[0]] = terms[2].strip()\n",
    "\n",
    "  return dict_cluss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXHWKZf8ImZp"
   },
   "outputs": [],
   "source": [
    "#Load Dictionaries\n",
    "\n",
    "def load_dict(file_name):\n",
    "  f = open(file_name)\n",
    "  adv_dict = []\n",
    "\n",
    "  for line in f:\n",
    "    adv_dict.append(line.strip())\n",
    "\n",
    "  return adv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWMsFFHOIr1n"
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "def load_data(f):\n",
    "  reviews = []\n",
    "\n",
    "  for line in f:\n",
    "    reviews.append(eval(line))\n",
    "\n",
    "  return reviews;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1Dyz2BRIzgJ"
   },
   "outputs": [],
   "source": [
    "#Extract Labels\n",
    "\n",
    "def extract_labels(reviews):\n",
    "  labels=[]\n",
    "\n",
    "  for review in reviews:\n",
    "    if(review[\"label\"] == \"Adverse\"):\n",
    "      labels.append(\"Adverse\")\n",
    "    else:\n",
    "      labels.append(\"Unknown\")\n",
    "    #labels.append(review[\"label\"])\n",
    "\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRdid-JSI0BI"
   },
   "outputs": [],
   "source": [
    "#Extract Entities\n",
    "\n",
    "def extract_entities(reviews):\n",
    "  entities = []\n",
    "\n",
    "  for review in reviews:\n",
    "    entities.append(review['entity'])\n",
    "\n",
    "  return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WUy935SI2xC"
   },
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "\n",
    "def extract_features_for_twitter_corpus(entities, is_train):\n",
    "  window_words = ExtractWindowWordsFeature().transform(entities)\n",
    "  pos_tag_features = numpy.array(PosTagFeatures().transform(window_words))\n",
    "  sentiment_feature = numpy.array(SentimentFeature().transform(window_words))\n",
    "  sentiment_score = numpy.array(SentimentScore().transform(window_words))\n",
    "  entities_text = ExtractEntitiesFeature().transform(entities)\n",
    "  w2v_feature = numpy.array(W2VFeatures(model=w2v_model, num_features=W2V_FEATURES_NUM).transform(entities_text))\n",
    "  umls_semantic_type_feature = numpy.array(UMLSemanticTypeFeature(umls_semantic_types).transform(entities_text))\n",
    "\n",
    "  if is_train:\n",
    "    X = vectorizer.fit_transform(window_words)\n",
    "  else:\n",
    "    X = vectorizer.transform(window_words)\n",
    "\n",
    "  X = X.toarray()\n",
    "  \n",
    "\n",
    "  features = numpy.concatenate((X, pos_tag_features), axis = 1)\n",
    "  features = numpy.concatenate((features, sentiment_feature), axis = 1)\n",
    "  features = numpy.concatenate((features, sentiment_score), axis = 1)\n",
    "  features = numpy.concatenate((features, umls_semantic_type_feature), axis = 1)\n",
    "  features = numpy.concatenate((features, w2v_feature), axis = 1)\n",
    "   \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaD1O0g_JLvU"
   },
   "outputs": [],
   "source": [
    "#SVM Classifier for Twitter Corpus\n",
    "\n",
    "def svm_classifier_for_twitter_corpus():\n",
    "  svc = LinearSVC(penalty='l2')\n",
    "  f_measure = []\n",
    "  entities = []\n",
    "  predicted = []\n",
    "  right = []\n",
    "  train_data = Bunch()\n",
    "  test_data = Bunch()\n",
    "\n",
    "  for i in range(1, 6):\n",
    "    f_train = open(folder_path + '/Twitter Corpus/' + str(i) + '/train.txt')\n",
    "    f_test = open(folder_path + '/Twitter Corpus/' + str(i) + '/test.txt')\n",
    "    train_data.reviews = load_data(f_train)\n",
    "    test_data.reviews = load_data(f_test)\n",
    "    train_data.labels = extract_labels(train_data.reviews)\n",
    "    test_data.labels = extract_labels(test_data.reviews)\n",
    "    train_data.entities = extract_entities(train_data.reviews)\n",
    "\n",
    "    features_train = extract_features_for_twitter_corpus(train_data.reviews, True)\n",
    "    \n",
    "    svc.fit(numpy.array(features_train), numpy.array(train_data.labels))\n",
    "\n",
    "    features_test = extract_features_for_twitter_corpus(test_data.reviews, False)\n",
    "\n",
    "    predicted_block = svc.predict(numpy.array(features_test))\n",
    "    predicted.extend(predicted_block)\n",
    "    right.extend(test_data.labels)\n",
    "\n",
    "    print(metrics.f1_score(test_data.labels, predicted_block, average='macro'))\n",
    "    f_measure.append(metrics.f1_score(test_data.labels, predicted_block, average='macro'))\n",
    "\n",
    "    entities.extend(test_data.reviews)\n",
    "\n",
    "  \n",
    "  print(str(f_measure))\n",
    "  print(\"\\033[1m\" + \"SVM Classification Report\")\n",
    "  print(classification_report(right, predicted, digits=3))\n",
    "  print (metrics.precision_score(right, predicted, average='macro'))\n",
    "  print (metrics.recall_score(right, predicted, average='macro'))\n",
    "  print (metrics.f1_score(right, predicted, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blN2V97Zj061"
   },
   "outputs": [],
   "source": [
    "#LR Classifier for Twitter Corpus\n",
    "\n",
    "def lr_classifier_for_twitter_corpus():\n",
    "  lr = LogisticRegression(penalty='l2')\n",
    "  f_measure = []\n",
    "  entities = []\n",
    "  predicted = []\n",
    "  right = []\n",
    "  train_data = Bunch()\n",
    "  test_data = Bunch()\n",
    "\n",
    "  for i in range(1, 6):\n",
    "    f_train = open(folder_path + '/Twitter Corpus/' + str(i) + '/train.txt')\n",
    "    f_test = open(folder_path + '/Twitter Corpus/' + str(i) + '/test.txt')\n",
    "    train_data.reviews = load_data(f_train)\n",
    "    test_data.reviews = load_data(f_test)\n",
    "    train_data.labels = extract_labels(train_data.reviews)\n",
    "    test_data.labels = extract_labels(test_data.reviews)\n",
    "    train_data.entities = extract_entities(train_data.reviews)\n",
    "\n",
    "    features_train = extract_features_for_twitter_corpus(train_data.reviews, True)\n",
    "    \n",
    "    lr.fit(numpy.array(features_train), numpy.array(train_data.labels))\n",
    "\n",
    "    features_test = extract_features_for_twitter_corpus(test_data.reviews, False)\n",
    "\n",
    "    predicted_block = lr.predict(numpy.array(features_test))\n",
    "    predicted.extend(predicted_block)\n",
    "    right.extend(test_data.labels)\n",
    "\n",
    "    print(metrics.f1_score(test_data.labels, predicted_block, average='macro'))\n",
    "    f_measure.append(metrics.f1_score(test_data.labels, predicted_block, average='macro'))\n",
    "\n",
    "    entities.extend(test_data.reviews)\n",
    "\n",
    "  \n",
    "  print(str(f_measure))\n",
    "  print(\"\\033[1m\" + \"Logistic regression Classification Report\")\n",
    "  print(classification_report(right, predicted, digits=3))\n",
    "  print (metrics.precision_score(right, predicted, average='macro'))\n",
    "  print (metrics.recall_score(right, predicted, average='macro'))\n",
    "  print (metrics.f1_score(right, predicted, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "id": "HyiqSaVMJRhM",
    "outputId": "f9e92d25-f83e-4dd1-9690-daae7c3e32f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 14\n",
      "0.841248303934871\n",
      "extract word transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 13\n",
      "0.6295793758480325\n",
      "extract word transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 11\n",
      "0.5821428571428571\n",
      "extract word transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 12\n",
      "0.7692307692307693\n",
      "extract word transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 12\n",
      "0.5989974937343359\n",
      "[0.841248303934871, 0.6295793758480325, 0.5821428571428571, 0.7692307692307693, 0.5989974937343359]\n",
      "\u001b[1mSVM Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Adverse      0.904     0.910     0.907       156\n",
      "     Unknown      0.548     0.531     0.540        32\n",
      "\n",
      "    accuracy                          0.846       188\n",
      "   macro avg      0.726     0.721     0.724       188\n",
      "weighted avg      0.844     0.846     0.845       188\n",
      "\n",
      "0.7264228477501541\n",
      "0.7207532051282051\n",
      "0.7235153912470206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  vectorizer = CountVectorizer(ngram_range = (1,2))\n",
    "  transformer = TfidfTransformer()\n",
    "  umls_semantic_types = load_umls_semantic_types()\n",
    "  w2v_model = KeyedVectors.load_word2vec_format(w2v_model_file, binary=True)\n",
    "  \n",
    "  \n",
    "  svm_classifier_for_twitter_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "znhXlpGFJ8rw",
    "outputId": "9575a343-9324-43c0-9110-ff7c838b24b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 14\n",
      "0.8852941176470589\n",
      "extract word transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 13\n",
      "0.6859903381642511\n",
      "extract word transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 11\n",
      "0.607645875251509\n",
      "extract word transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 12\n",
      "0.7450980392156863\n",
      "extract word transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract word transform\n",
      "Sentiment score = 12\n",
      "0.4666666666666667\n",
      "[0.8852941176470589, 0.6859903381642511, 0.607645875251509, 0.7450980392156863, 0.4666666666666667]\n",
      "\u001b[1mLogistic regression Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Adverse      0.902     0.942     0.922       156\n",
      "     Unknown      0.640     0.500     0.561        32\n",
      "\n",
      "    accuracy                          0.867       188\n",
      "   macro avg      0.771     0.721     0.742       188\n",
      "weighted avg      0.857     0.867     0.860       188\n",
      "\n",
      "0.770920245398773\n",
      "0.7211538461538461\n",
      "0.7415168014079085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "lr_classifier_for_twitter_corpus()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "IR_Project_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
